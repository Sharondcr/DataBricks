{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ddebad-9c8b-4ec8-a828-f94f7d83bc77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting azure-storage-blob\n  Downloading azure_storage_blob-12.21.0-py3-none-any.whl (396 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 396.4/396.4 kB 3.6 MB/s eta 0:00:00\nCollecting isodate>=0.6.1\n  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.7/41.7 kB 6.5 MB/s eta 0:00:00\nCollecting azure-core>=1.28.0\n  Downloading azure_core-1.30.2-py3-none-any.whl (194 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.3/194.3 kB 17.6 MB/s eta 0:00:00\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-blob) (39.0.1)\nCollecting typing-extensions>=4.6.0\n  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core>=1.28.0->azure-storage-blob) (1.16.0)\nRequirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.10/site-packages (from azure-core>=1.28.0->azure-storage-blob) (2.28.1)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.10/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.1)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (2022.12.7)\nInstalling collected packages: typing-extensions, isodate, azure-core, azure-storage-blob\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.4.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-b885cdb1-4633-4bed-a823-79b8bd5dd970\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\nSuccessfully installed azure-core-1.30.2 azure-storage-blob-12.21.0 isodate-0.6.1 typing-extensions-4.12.2\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-storage-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da19e24e-e04a-4cb3-9d0a-0fa78ffda843",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- EmployeeID: integer (nullable = true)\n |-- Department: string (nullable = true)\n |-- JobRole: string (nullable = true)\n |-- Attrition: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- MaritalStatus: string (nullable = true)\n |-- Education: string (nullable = true)\n |-- EducationField: string (nullable = true)\n |-- BusinessTravel: string (nullable = true)\n |-- JobInvolvement: string (nullable = true)\n |-- JobLevel: integer (nullable = true)\n |-- JobSatisfaction: string (nullable = true)\n |-- Hourlyrate: integer (nullable = true)\n |-- Income: integer (nullable = true)\n |-- Salaryhike: integer (nullable = true)\n |-- OverTime: string (nullable = true)\n |-- Workex: integer (nullable = true)\n |-- YearsSinceLastPromotion: integer (nullable = true)\n |-- EmpSatisfaction: string (nullable = true)\n |-- TrainingTimesLastYear: integer (nullable = true)\n |-- WorkLifeBalance: string (nullable = true)\n |-- Performance_Rating: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=datalakedemo001;AccountKey=ndrhiH6jpzITegf7mGnuFB14JGKcDJqSedz/QeIoFxRRpCbql9r5TtDn94WrsC2i5M9Vqv5Puylm+ASt/s0Akw==;EndpointSuffix=core.windows.net\"\n",
    "container_name = \"classwork\"\n",
    "blob_name = \"HR_Employee.csv\"\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AzureBlobSparkReadStream\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set the Azure Storage account key\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.datalakedemo001.blob.core.windows.net\",\n",
    "    \"ndrhiH6jpzITegf7mGnuFB14JGKcDJqSedz/QeIoFxRRpCbql9r5TtDn94WrsC2i5M9Vqv5Puylm+ASt/s0Akw==\"\n",
    ")\n",
    "blob_url = f\"wasbs://{container_name}@{connection_string.split(';')[1].split('=')[1]}.blob.core.windows.net/{blob_name}\"\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(blob_url)\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file as a streaming DataFrame\n",
    "df1 = spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(df.schema) \\\n",
    "    .load(blob_url)\n",
    "\n",
    "# # Display the streaming DataFrame\n",
    "df1.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "045fe751-ef03-4afa-99ef-17e40fccfb50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Perform basic transformations\n",
    "df2 = df1.filter(col(\"age\") >= 30)\n",
    "df2 = df2.withColumnRenamed(\"department\", \"dept\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "923de602-9cb3-43b1-9394-b8d0e35bba22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3 = df2.groupBy(\"dept\").agg(\n",
    "    sum(\"Income\").alias(\"total_salary\"),\n",
    "    avg(\"Age\").alias(\"average_age\"),\n",
    "    count(\"*\").alias(\"employee_count\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96c05062-7d99-4266-b7fd-4dabd7398071",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the Cosmos DB connection details\n",
    "cosmos_db_endpoint = \"https://classworks.documents.azure.com:443/\"\n",
    "cosmos_db_master_key = \"xHoCCouttJqTLi0OWjpfptC0RqyccltDPi6lFbhtaq1jUbcrXD3bnnXMoIG2rDtlfDnb0CvfoEK9ACDbnmuoNQ==\"\n",
    "cosmos_db_database = \"BlobToDb\"\n",
    "cosmos_db_collection = \"ContainerCW\"\n",
    "\n",
    "# Define the Cosmos DB sink\n",
    "cosmos_db_sink = spark._jvm.com.microsoft.azure.cosmosdb.spark.config.Config(\n",
    "    endpoint=cosmos_db_endpoint,\n",
    "    masterKey=cosmos_db_master_key,\n",
    "    database=cosmos_db_database,\n",
    "    collection=cosmos_db_collection\n",
    ")\n",
    "\n",
    "# Write the aggregated DataFrame to Cosmos DB\n",
    "df3.writeStream \\\n",
    "    .format(\"cosmosdb\") \\\n",
    "    .options(**cosmos_db_sink) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint\") \\\n",
    "    .start()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "blob to cosmo",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
